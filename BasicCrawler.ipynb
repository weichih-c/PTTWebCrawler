{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# from utils import pretty_print\n",
    "\n",
    "INDEX = 'https://www.ptt.cc/bbs/Boy-Girl/index.html'\n",
    "\n",
    "def get_posts_on_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    posts = list()\n",
    "    for article in soup.find_all('div', 'r-ent'):\n",
    "        meta = article.find('div', 'title').find('a')\n",
    "        if meta:\n",
    "            title = meta.getText().strip()\n",
    "            if \"[公告]\" in title or \"置底\" in title: continue\n",
    "            posts.append({\n",
    "                'title': title,\n",
    "                'link': meta.get('href'),\n",
    "                'push': article.find('div', 'nrec').getText(),\n",
    "                'date': article.find('div', 'date').getText(),\n",
    "                'author': article.find('div', 'author').getText(),\n",
    "            })\n",
    "\n",
    "    next_link = soup.find('div', 'btn-group-paging').find_all('a', 'btn')[1].get('href')\n",
    "\n",
    "    return posts, next_link\n",
    "\n",
    "\n",
    "def get_pages(num):\n",
    "    page_url = INDEX\n",
    "    all_posts = list()\n",
    "    for i in range(num):\n",
    "        posts, link = get_posts_on_page(page_url)\n",
    "        all_posts += posts\n",
    "        page_url = urllib.parse.urljoin(INDEX, link)\n",
    "        \n",
    "    return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_content(post):\n",
    "    article_info = dict() # used for output\n",
    "    pusherList = list() # get pushers in the post\n",
    "\n",
    "    articleLink = urllib.parse.urljoin(INDEX, post['link'])\n",
    "    response = requests.get(articleLink)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    mainContent = soup.find('div', id='main-container').find('div',id='main-content')\n",
    "\n",
    "    # record pusher and remove pushes\n",
    "    pushes = mainContent.find_all('div','push')\n",
    "    for p in pushes:\n",
    "        uid = p.find('span', 'push-userid').getText()\n",
    "        pusherList.append(uid)\n",
    "        p.decompose()\n",
    "\n",
    "    # get time and remove metas\n",
    "    metas = mainContent.find_all('div', 'article-metaline')\n",
    "    articleTime = metas[2].find('span','article-meta-value').getText()\n",
    "    for meta in metas:\n",
    "        meta.decompose()\n",
    "        \n",
    "    header = mainContent.find('div', 'article-metaline-right')\n",
    "    if header: header.decompose()\n",
    "\n",
    "    # remove noise\n",
    "    noise = mainContent.find_all('span')\n",
    "    for item in noise: item.decompose()\n",
    "    articleContent = mainContent.getText().strip()\n",
    "    \n",
    "    article_info[\"title\"] = post['title']\n",
    "    article_info[\"push\"] = post['push']\n",
    "    article_info[\"author\"] = post['author']\n",
    "    article_info[\"datetime\"] = articleTime\n",
    "    article_info[\"content\"] = articleContent\n",
    "    article_info[\"pusher\"] = pusherList\n",
    "    return article_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def get_articles(postList):\n",
    "    with Pool(processes=8) as pool:\n",
    "        article_info = pool.map(fetch_article_content, postList)\n",
    "        return article_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花費: 5.252650 秒\n",
      "共77項結果\n",
      "花費: 0.007368 秒做sorting\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pages = 5\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    postList = get_pages(pages)\n",
    "    article_infos = get_articles(postList)\n",
    "    \n",
    "    print('花費: %f 秒' % (time.time() - start))\n",
    "\n",
    "    print('共%d項結果' % len(article_infos))\n",
    "    \n",
    "    # sorting by datetimet\n",
    "    start = time.time()\n",
    "    article_infos = sorted(article_infos, key=lambda article: datetime.strptime(article['datetime'], '%a %b %d %H:%M:%S %Y'), reverse=True )\n",
    "    print('花費: %f 秒做sorting' % (time.time() - start))\n",
    "\n",
    "\n",
    "    outputDir_csv = \"Output/csv/\"\n",
    "    if not os.path.isdir(outputDir_csv): os.makedirs(outputDir_csv)\n",
    "    outputDir_txt = \"Output/txt/\"\n",
    "    if not os.path.isdir(outputDir_txt): os.makedirs(outputDir_txt)\n",
    "        \n",
    "    docID = 1\n",
    "    for article in article_infos:\n",
    "        \n",
    "        # write csv file\n",
    "        with open(outputDir_csv + str(docID) + '.csv', 'w', newline='',encoding='utf-8-sig') as f:\n",
    "            spamwriter = csv.writer(f, delimiter=',',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            header = [\"Author\", \"Title\", \"Push\", \"Time\", \"Content\"]\n",
    "            spamwriter.writerow(header)\n",
    "            row = (article['author'], article['title'], article['push'], article['datetime'], article['content'])\n",
    "            spamwriter.writerow(row)\n",
    "            f.close()\n",
    "            \n",
    "        # write txt file\n",
    "        with open(outputDir_txt + str(docID) + '.txt', 'w') as f:\n",
    "            f.write(\"Author:\"+'\\t'+article['author']+'\\n')\n",
    "            f.write(\"Title:\"+'\\t'+article['title']+'\\n')\n",
    "            f.write(\"Push:\"+'\\t'+article['push']+'\\n')\n",
    "            f.write(\"Time:\"+'\\t'+article['datetime']+'\\n')\n",
    "            f.write(\"\\nContent:\\n\"+article['content']+'\\n')\n",
    "        docID += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
